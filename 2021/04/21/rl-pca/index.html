<!DOCTYPE HTML>
<html lang="zh-CN,en,default">
<head>
    <head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="java,android,music,emotion,Chitose Yono, hexo-theme-matery">
    <meta name="description" content="我所追寻的自己，可能不存在梦里">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>White Noise -Chitose Yono Offical Blog-</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/gitment/gitment-default.css">
    <link rel="stylesheet" type="text/css" href="/libs/gitment/gitment.js">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/gitment.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>
</head>

<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span"></span>
                    </a>
                </div>

                <a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">Index</a>
    </li>
    
    
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">Tags</a>
    </li>
    
    
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">Archives</a>
    </li>
    
    
    
    <li class="hide-on-med-and-down">
        <a href="/guestbook" class="waves-effect waves-light">Guestbook</a>
    </li>
    
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">
    <div class="mobile-head bg-color">
        
        <img src="/medias/sublogo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">White Noise</div>
        <div class="logo-desc">
            
            我所追寻的自己，可能不存在梦里
            
        </div>
    </div>
    <ul class="menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                <i class="fa fa-link fa-lg fa-fw"></i>Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                <i class="fa fa-link fa-lg fa-fw"></i>Tags
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                <i class="fa fa-link fa-lg fa-fw"></i>Archives
            </a>
        </li>
        
        <li>
            <a href="/guestbook" class="waves-effect waves-light">
                <i class="fa fa-link fa-lg fa-fw"></i>Guestbook
            </a>
        </li>
        
    </ul>
    <div class="social-link">
        <a href="https://github.com/chitoseyono" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub"
           data-position="top" data-delay="50">
            <i class="fa fa-github fa-lg"></i>
        </a>
        <a href="mailto:chitoseyono@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我"
           data-position="top" data-delay="50">
            <i class="fa fa-envelope fa-lg"></i>
        </a>
        <a href="https://weibo.com/2818513061/profile" class="tooltipped" data-tooltip="访问我的微博"
           data-position="top" data-delay="50">
            <i class="fa fa-weibo fa-lg"></i>
        </a>
    </div>
</div>

            </div>
        </div>
    </nav>
</header>



<div class="bg-cover post-cover" style="background-image: url('/medias/images/rl_pca.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        <font size="6">表征学习、以及PCA的细节公式推导</font>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<main class="content">

    <!-- 目录内容 -->
         

    <!-- 文章内容详情 -->
    <div id="artDetail" class="container">
        <div class="card">
            <div class="card-content article-info">
                
                <div class="article-tag">
                    
                    <a href="https://chitoseyono.com/tags/机器学习/" target="_blank"><span class="chip bg-color">机器学习</span></a>
                    
                </div>
                
                <div class="author-info">
                    <span>
                        <i class="fa fa-calendar fa-fw"></i>2021-04-21
                    </span>
                </div>
            </div>
            <hr>
            <div class="card-content article-card-content">
                <div id="articleContent">
                    <script src="/assets/js/APlayer.min.js"> </script><h1 id="Representation-Learning-表征学习"><a href="#Representation-Learning-表征学习" class="headerlink" title="Representation Learning 表征学习"></a>Representation Learning 表征学习</h1><blockquote>
<p>这章将稍微讲讲表征学习，以及经典方法PCA。</p>
<p>机器学习算法的与否可行不仅仅取决于算法的正确选用，也取决于数据的质量和有效的数据表示（representation）。针对不同类型的数据（text，image，video），错误的表示方式可能会导致有效信息的缺失或是暴露，这决定了算法能否有效地解决问题。表征学习的目的是对复杂的原始数据化繁为简，把原始数据的无效的或者冗余的信息剔除，把有效信息进行提炼，形成特征（feature）。特征提取可以人为地手工处理，也可以借助特定的算法自动提取。Roughly Speaking， 前者（手动处理）称为特征工程，后者（借助算法）为表征学习（Representation Learning）。如果数据量较小，我们可以根据自身的经验和先验知识，人为地设计出合适的特征，用作下游的任务，比如分类；但数据量很大且复杂时，则需要依赖自动化的表征学习。</p>
<p>注意，这之后的章节内容都可能比较混乱/缺失，我尽量会把算法的关键思想说清，但是公式推导会有点跳步，请尽量搭配对每一章应参考资料查阅。</p>
<ol>
<li>表征学习：<a href="https://zhuanlan.zhihu.com/p/136554341" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/136554341</a></li>
<li>流形学习：<a href="https://scikit-learn.org/stable/modules/manifold.htm" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/manifold.htm</a></li>
<li>稀疏表示，特征提取，特征选择：西瓜书</li>
<li>van der Maaten, Laurens &amp; Postma, Eric &amp; Herik, H.. (2007). Dimensionality Reduction: A Comparative Review. Journal of Machine Learning Research - JMLR. 10. </li>
<li>Anowar, Farzana &amp; Sadaoui, Samira &amp; Selim, Bassant. (2021). Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE). Computer Science Review. 40. 10.1016/j.cosrev.2021.100378. </li>
</ol>
</blockquote>
<p>Representation Learning要做的一般有：</p>
<ul>
<li>Dimension Reduction 降维</li>
<li>Manifold Learning 流式学习</li>
<li>Sparse Representation 稀疏表示</li>
</ul>
<p><strong>目的</strong>：</p>
<ul>
<li>减少算法训练/学习的开销，避免<strong>维数灾难</strong></li>
<li>让数据更好可视化，更好理解与Debug</li>
<li>降低数据存储</li>
</ul>
<h3 id="The-curse-of-dimensionality-维数灾难"><a href="#The-curse-of-dimensionality-维数灾难" class="headerlink" title="The curse of dimensionality 维数灾难"></a>The curse of dimensionality 维数灾难</h3><blockquote>
<p>在kNN上，我们发现任意测试样本 $x$ 附近任意小的 $δ$ 距离范围内，总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样”(dense sample)。然而，这个假设在现实任务中通常很难满足，例如 $δ=0.001$，仅考虑单个属性，则仅需1000个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近0.001距离范围内总能找到一个训练样本。此时最近邻分类器的错误率不超过贝叶斯最优分类器的错误率的两倍。然而，这仅是属性维数为1的情形。若有更多的属性，则情况会发生显著变化。例如假定属性维数为20，若要求样本满足密采样条件，则至少需要$1000^{20}=10^{60}$个样本。现实应用中属性维数经常成千上万，要满足密采样条件约为所需的样本数目是无法达到的天文数字。此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大的麻烦，当维数很高时甚至连计算内积都不再容易。</p>
<p>事实上，在高位情形下出现的数据样本稀疏、距离计算困难问题，是所有机器学习方法的共通障碍——<strong>维数灾难</strong>。</p>
</blockquote>
<h3 id="Feature-Extraction-vs-Feature-Selection-特征提取-vs-特征选择"><a href="#Feature-Extraction-vs-Feature-Selection-特征提取-vs-特征选择" class="headerlink" title="Feature Extraction vs Feature Selection 特征提取 vs 特征选择"></a>Feature Extraction vs Feature Selection 特征提取 vs 特征选择</h3><p><img src="/2021/04/21/rl-pca/features_change.png" alt="features"></p>
<p><strong>Feature Extraction</strong> are ways to transform/project the original features in the data into new features which have some advantage such as</p>
<ul>
<li>Lower Dimensionality</li>
<li>Better description of the variance in the data</li>
<li>Better ability to distinguish data points or clusters of data points</li>
</ul>
<p>Whereas <strong>Feature Selection</strong> is attempt to find a subset of the original features which <strong>satisfy some criteria</strong>.  Ideally the selected subset includes the <strong>significant features</strong> and <strong>eliminates irrelevant and redundant</strong> features.</p>
<p>Methods of Feature Selection</p>
<ul>
<li>Feature Ranking</li>
<li>Filter Approach, 如 Relevant Features (Relief)</li>
<li>Wrapper approach, 如 Las Vegas Wrapper (LVW)</li>
<li>Embedding approach</li>
</ul>
<p>This Chapter is focus on <strong>Feature Extraction</strong>, though feature selection is needed to learned as when comes to some data analysis tasks.</p>
<p><strong>Methods of Feature Extraction</strong></p>
<ul>
<li>Linear Methods:<ul>
<li>Unsupervised<ul>
<li>PCA</li>
<li>ICA</li>
</ul>
</li>
<li>Supervised<ul>
<li>LDA</li>
</ul>
</li>
</ul>
</li>
<li>Non-linear Methods:<ul>
<li>Global <em>(preserve global properties)</em><ul>
<li>MDS</li>
<li>Isomap</li>
<li>Kernel PCA</li>
</ul>
</li>
<li>Local <em>(preserve properties within local neighborhood)</em><ul>
<li>LLE</li>
</ul>
</li>
<li>Global + Local<ul>
<li>SNE</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>简单比较他们的性质：</p>
<p><img src="/2021/04/21/rl-pca/comparison1.png" alt="compare1"></p>
<p><img src="/2021/04/21/rl-pca/comparison2.png" alt="compare2"></p>
<h1 id="Principal-Component-Analysis-主成分分析"><a href="#Principal-Component-Analysis-主成分分析" class="headerlink" title="Principal Component Analysis 主成分分析"></a>Principal Component Analysis 主成分分析</h1><blockquote>
<p>参考资料：</p>
<ol>
<li>西瓜书 10.3</li>
<li>拉格朗日函数：<a href="https://math.stackexchange.com/questions/1104376/how-to-set-up-lagrangian-optimization-with-matrix-constrains" target="_blank" rel="noopener">https://math.stackexchange.com/questions/1104376/how-to-set-up-lagrangian-optimization-with-matrix-constrains</a></li>
<li>Unsupervised and Supervised Principal Component Analysis: Tutorial</li>
<li>花书 2.7, 2.8</li>
<li>M. Turk and A. Pentland, “Eigenfaces for recognition,” Journal of cognitive neuroscience, vol. 3, no. 1, pp. 71–86, 1991.</li>
<li>M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,” in Computer Vision and Pattern Recognition, 1991. Proceedings CVPR’91., IEEE Computer Society Conference on, pp. 586–591, IEEE, 1991.</li>
<li>视觉化：<a href="https://setosa.io/ev/principal-component-analysis/" target="_blank" rel="noopener">https://setosa.io/ev/principal-component-analysis/</a></li>
</ol>
</blockquote>
<p>PCA降维的核心思想是：找到最有影响力，使全部样本最大可分的向量（即方差最大的），作为主成分，然后映射到该向量去。对其求垂直便能找到第二个主成分，然后找到k个主成分后，那么原来n维可以降到k维。</p>
<p>主要做法有Eigen-decomposition（特征值分解）与Singular Value Decomposition（奇异值分解），后者被称为Dual PCA。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="Preprocessing-预处理"><a href="#Preprocessing-预处理" class="headerlink" title="Preprocessing 预处理"></a>Preprocessing 预处理</h3><p><strong>Train Set</strong>：$\mathbf{X}\in \R^{d*n} = [X_1,\cdots,X_n] = {X_i\in \R^d}_{i=1}^n$ </p>
<p><strong>Mean</strong>：$\mu=\frac{1}{n}\sum^n_{i=1} X_i \in \R^d$</p>
<p><strong>Normalization</strong>：$\breve{\mathbf{X}}=\mathbf{X}-\mu =\mathbf{X}·H$（这里是用线性变换的方法近似归一化效果，减少运算量，其中$H=I-(1/n)11^T$，$1$是全1向量。$H$被称为 <strong><a href="https://en.wikipedia.org/wiki/Centering_matrix" target="_blank" rel="noopener">centering matrix</a></strong>）</p>
<p>（其中对于NLP的任务不需要归一化，因为NLP中负数是无意义的，这时就称为LSI/LSA）</p>
<h3 id="投影-Projection-与-还原-Reconstruction"><a href="#投影-Projection-与-还原-Reconstruction" class="headerlink" title="投影 Projection 与 还原 Reconstruction"></a>投影 Projection 与 还原 Reconstruction</h3><p>我们的目的是找到映射矩阵 $\mathbf{U}\in\R^{d<em>p}$，将数据从 $\R^{d</em>n}$ 映射到 $\R^{p*n}$。</p>
<p><strong>Projection</strong>：$\mathbf{\tilde{X}} = \mathbf{U}^T\mathbf{\breve{X}},\mathbf{\tilde{X}}\in \R^{p*n}$</p>
<p><strong>Reconstruction</strong>：$\mathbf{\hat{X}}=\mathbf{U}\mathbf{U}^T\mathbf{\breve{X}}+\mu=\mathbf{U}\mathbf{\tilde{X}}+\mu$</p>
<p><strong>Test Set：</strong>$\mathbf{\tilde{X_t}} = \mathbf{U}^T\mathbf{\breve{X}},\mathbf{\hat{X_t}}=\mathbf{U}\mathbf{\tilde{X_t}}+\mu_x$（相当于用训练出的投影矩阵处理测试集）</p>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>为寻找优化目标，我们从两个角度去看主成分分析算法。分别是<strong>最小化重构Error（最近重构性）</strong>和<strong>最大方差（最大可分性）</strong>，他们能够得出一样的结果。</p>
<h3 id="1-最大化方差"><a href="#1-最大化方差" class="headerlink" title="1 最大化方差"></a>1 最大化方差</h3><p>根据最大可分性，我们希望重构后数据的方差会尽量的大。我们将作为constant项的$\mu$去掉，取其<strong>Square Frobenius Norm</strong>，有：<br>$$<br>\begin{array}{l}<br>\left|\hat{\mathbf{X}}\right|<em>{F}^{2} = \left|\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right|</em>{F}^{2} \<br>=\operatorname{tr}\left(\left(\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right)^{\top}\left(\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right)\right) \<br>=\operatorname{tr}(\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \underbrace{\boldsymbol{U}^{\top} \boldsymbol{U}}_{\boldsymbol{I}} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}) \<br>=\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right) \<br>=\operatorname{tr}\left(\boldsymbol{U}^{\top}\breve{\boldsymbol{X}}\breve{\boldsymbol{X}}^{\top} \boldsymbol{U}  \right)<br>\end{array}<br>$$<br><strong>Minimization</strong>（常规做法会用到拉格朗日函数，最好查看参考链接[3]理解这一步）：<br>$$<br>\begin{aligned}<br>&amp;\underset{\boldsymbol{U}}{\operatorname{minimize}}\operatorname{tr}\left(\boldsymbol{U}^{\top}\breve{\boldsymbol{X}}\breve{\boldsymbol{X}}^{\top} \boldsymbol{U}  \right)  \<br>&amp;\text { subject to } \boldsymbol{U}^{\top} \boldsymbol{U}=\boldsymbol{I} \<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathcal{L}=\operatorname{tr}\left(\boldsymbol{U}^{\top}\breve{\boldsymbol{X}}\breve{\boldsymbol{X}}^{\top} \boldsymbol{U}  \right) -\operatorname{tr}\left(\Lambda^{\top}\left(\boldsymbol{U}^{\top} \boldsymbol{U}-\boldsymbol{I}\right)\right) \<br>\mathbb{R}^{d \times p} \ni \frac{\partial \mathcal{L}}{\partial \boldsymbol{U}}=2 \breve{\boldsymbol{X}}\breve{\boldsymbol{X}}^{\top} \boldsymbol{U}-2 \boldsymbol{U} \Lambda \stackrel{\text { set }}{=} 0 \\Longrightarrow \breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} \boldsymbol{U}=\boldsymbol{U} \Lambda<br>$$</p>
<h3 id="2-最小化重构Error"><a href="#2-最小化重构Error" class="headerlink" title="2 最小化重构Error"></a>2 最小化重构Error</h3><p>根据最近重构性，我们希望重构后的原数据会尽量相似，那就指出了我们的优化目标：<strong>最小化重构/还原Error(即距离)：</strong></p>
<p><strong>Reconstruction Distance</strong>：<br>$$<br>\begin{array}{l}<br>\left|\mathbf{X}-\mathbf{\hat{X}}\right|<em>{F}^{2} = \left|\breve{\boldsymbol{X}}-\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right|</em>{F}^{2} \<br>=\operatorname{tr}\left(\left(\breve{\boldsymbol{X}}-\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right)^{\top}\left(\breve{\boldsymbol{X}}-\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right)\right) \<br>=\operatorname{tr}\left(\left(\breve{\boldsymbol{X}}^{\top}-\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top}\right)\left(\breve{\boldsymbol{X}}-\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right)\right) \<br>=\operatorname{tr}(\breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}-2 \breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}+\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \underbrace{\boldsymbol{U}^{\top} \boldsymbol{U}}<em>{\boldsymbol{I}} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}) \<br>=\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}-\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right) \<br>=\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}\right)-\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right) \<br>=\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}\right)-\operatorname{tr}\left(\breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top}\right)<br>\end{array}<br>$$<br><strong>Minimization</strong>：<br>$$<br>\begin{aligned}<br>&amp;\underset{\boldsymbol{U}}{\operatorname{minimize}}\left|\breve{\boldsymbol{X}}-\boldsymbol{U} \boldsymbol{U}^{\top} \breve{\boldsymbol{X}}\right|</em>{F}^{2} \<br>&amp;\text { subject to } \boldsymbol{U}^{\top} \boldsymbol{U}=\boldsymbol{I} \<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathcal{L}=\operatorname{tr}\left(\breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}\right)-\operatorname{tr}\left(\breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} \boldsymbol{U} \boldsymbol{U}^{\top}\right)-\operatorname{tr}\left(\Lambda^{\top}\left(\boldsymbol{U}^{\top} \boldsymbol{U}-\boldsymbol{I}\right)\right) \<br>\mathbb{R}^{d \times p} \ni \frac{\partial \mathcal{L}}{\partial \boldsymbol{U}}=2 \breve{\boldsymbol{X}}\breve{\boldsymbol{X}}^{\top} \boldsymbol{U}-2 \boldsymbol{U} \Lambda \stackrel{\text { set }}{=} 0 \\Longrightarrow \breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} \boldsymbol{U}=\boldsymbol{U} \Lambda<br>$$</p>
<blockquote>
<p><strong>注释</strong></p>
<p>对于带矩阵约束的优化问题，通过<a href="https://math.stackexchange.com/questions/1104376/how-to-set-up-lagrangian-optimization-with-matrix-constrains" target="_blank" rel="noopener">拉格朗日乘子</a>可以求解。</p>
<p>e.g. 对于$\text{max或min} f(X) \ s.t. Φ(X)=0$，其拉格朗日函数为$\mathcal L(X,\Lambda) = f(X)+tr(\Lambda^{\top}Φ(X))$，具体请查阅 [3]。</p>
</blockquote>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>上述两个方法最后得出的式子都是一个：$\breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} \boldsymbol{U}=\boldsymbol{U} \Lambda$。</p>
<p>至此，我们只需要对$\breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top}$进行特征值分解$\breve{\boldsymbol{X}} \breve{\boldsymbol{X}}^{\top} =\boldsymbol{U} \Lambda\boldsymbol{U}^{\top}$，将求得的特征值$\Lambda$从小到大进行排序:$\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_d$，然后取前$p$个特征值对应构成特征向量$\mathbf{U} = [u_1,\cdots,u_p]$，其中$u_k$为单位特征向量，便是我们最后得到的<strong>投影矩阵</strong>了。</p>
<p>一般如何选择需要降的维数$p$呢？</p>
<h3 id="Scree-plot-Ratio"><a href="#Scree-plot-Ratio" class="headerlink" title="Scree plot, Ratio"></a>Scree plot, Ratio</h3><p>$$<br>ratio = \frac{\lambda_j}{\sum^d_{k=1}\lambda_k}<br>$$</p>
<p><img src="/2021/04/21/rl-pca/scree_plot.png" alt="screeplot"></p>
<p>一般没有要求的话，我们会选择ratio的转折点(knee)的那一点作为我们最后选择的p。对重构阈值$t$有要求的话就：<br>$$<br>\frac{\sum^p_{k=1}\lambda_k}{\sum^d_{k=1}\lambda_k}\ge t<br>$$<br>这样就OK啦。</p>
<h2 id="Dual-PCA"><a href="#Dual-PCA" class="headerlink" title="Dual PCA"></a>Dual PCA</h2><p>如果我们不用特征值分解呢？而是使用 奇异值分解 SVD 呢？这时候就是Dual PCA。</p>
<h3 id="Why-Dual-PCA"><a href="#Why-Dual-PCA" class="headerlink" title="Why Dual PCA?"></a>Why Dual PCA?</h3><ol>
<li>在对于n&lt;&lt;d的计算时，对$ \boldsymbol{\breve{X}}^{\top}\boldsymbol{\breve{X}} $的特征值分解比对$\boldsymbol{\breve{X}} \boldsymbol{\breve{X}}^{\top} $的特征值分解要快很多（因为$\boldsymbol{\breve{X}}^{\top}\boldsymbol{\breve{X}} \in\R^{n\times n}$而$\boldsymbol{\breve{X}}\boldsymbol{\breve{X}}^{\top} \in\R^{p\times p}$。</li>
<li>对于Kernel PCA的形式上有帮助。（Kernel PCA在之后有时间会尝试补充）</li>
</ol>
<h3 id="奇异值分解-SVD-Review"><a href="#奇异值分解-SVD-Review" class="headerlink" title="奇异值分解 SVD Review"></a>奇异值分解 SVD Review</h3><p>奇异值分解的形式都一样：$\mathbf{A} = \mathbf{U\Sigma V}^\top,\mathbf{A}\in \R^{a\times b}$</p>
<p>SVD 本身也可以用于降维，当原数据有zero mean时其实就是PCA在用SVD了。</p>
<p>奇异值分解有两种：</p>
<h4 id="Complete-SVD"><a href="#Complete-SVD" class="headerlink" title="Complete SVD"></a>Complete SVD</h4><p>$\mathbf{U}\in\R^{a\times a},\mathbf{V}\in\R^{b\times b},\mathbf{\Sigma}\in\R^{a\times b}$</p>
<p>此时$\mathbf{U}$称为左奇异向量，$\mathbf{V}$称为右奇异向量，$\mathbf{\Sigma}$是一个矩形矩阵，其<strong>主对角线</strong>上的值都是奇异值。<br>$$<br>\boldsymbol{\Sigma}=\left[\begin{array}{ccc}<br>\sigma_{1} &amp; 0 &amp; 0 \<br>\vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; \sigma_{\beta} \<br>0 &amp; 0 &amp; 0 \<br>\vdots &amp; \vdots &amp; \vdots \<br>0 &amp; 0 &amp; 0<br>\end{array}\right] \text { and }\left[\begin{array}{cccccc}<br>\sigma_{1} &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \<br>\vdots &amp; \ddots &amp; \vdots &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; 0 &amp; \sigma_{\alpha} &amp; 0 &amp; \cdots &amp; 0<br>\end{array}\right]<br>$$<br>其中奇异值数量是$min(a,b)$。</p>
<h4 id="Incomplete-SVD"><a href="#Incomplete-SVD" class="headerlink" title="Incomplete SVD"></a>Incomplete SVD</h4><p>$\mathbf{U}\in\R^{a\times k},\mathbf{V}\in\R^{b\times k},\mathbf{\Sigma}\in\R^{k\times k}$ where $k:=min(a,b)$</p>
<p>此时$\mathbf{U}$称为左奇异向量，$\mathbf{V}$称为右奇异向量，$\mathbf{\Sigma}$是一个方正矩阵，其<strong>主对角线</strong>上的值都是奇异值。<br>$$<br>\boldsymbol{\Sigma}=\left[\begin{array}{ccc}<br>\sigma_{1} &amp; 0 &amp; 0 \<br>\vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; \sigma_{k} \<br>\end{array}\right]<br>$$</p>
<h4 id="SVD的特点"><a href="#SVD的特点" class="headerlink" title="SVD的特点"></a>SVD的特点</h4><p>首先，无论是Complete还是Incomplete，其$\mathbf{U,V}$都<strong>满足正交</strong>，即：<br>$$<br>s.t. \ \boldsymbol{U}^{\top} \boldsymbol{U}=\boldsymbol{I}\<br>s.t. \ \boldsymbol{V}^{\top} \boldsymbol{V}=\boldsymbol{I}<br>$$<br>其中$\mathbf{U}$是的特征分解后的特征向量，而$\mathbf{V}$为的特征值分解后的特征向量，证明如下。<br>$$<br>\begin{aligned}<br>\boldsymbol{A} \boldsymbol{A}^{\top} &amp;=\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}\right)\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}\right)^{\top}=\boldsymbol{U} \boldsymbol{\Sigma} \underbrace{\boldsymbol{V}^{\top} \boldsymbol{V}}_{\boldsymbol{I}} \boldsymbol{\Sigma} \boldsymbol{U}^{\top} \<br>&amp;=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{\Sigma} \boldsymbol{U}^{\top}=\boldsymbol{U} \boldsymbol{\Sigma}^{2} \boldsymbol{U}^{\top}<br>\end{aligned}\</p>
<p>\begin{aligned}<br>\boldsymbol{A}^{\top} \boldsymbol{A} &amp;=\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}\right)^{\top}\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}\right)=\boldsymbol{V} \boldsymbol{\Sigma} \underbrace{\boldsymbol{U}^{\top} \boldsymbol{U}}_{\boldsymbol{I}} \boldsymbol{\Sigma} \boldsymbol{V}^{\top} \<br>&amp;=\boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}=\boldsymbol{V} \boldsymbol{\Sigma}^{2} \boldsymbol{V}^{\top}<br>\end{aligned}<br>$$</p>
<h3 id="在PCA中使用SVD"><a href="#在PCA中使用SVD" class="headerlink" title="在PCA中使用SVD"></a>在PCA中使用SVD</h3><p>在这里，我们使用Incomplete SVD来做完原来PCA所做的所有内容，但我们将$\mathbf{U}$全部转换为$\mathbf{V}$：</p>
<p>$\mathbf{\breve{X}} = \mathbf{U\Sigma V}^\top$, where $\mathbf{U}\in\R^{d\times p},\mathbf{V}\in\R^{n\times p},\mathbf{\Sigma}\in\R^{p\times p}$</p>
<h4 id="1-Projection"><a href="#1-Projection" class="headerlink" title="1.Projection"></a><strong>1.Projection</strong></h4><p>$$<br>\mathbf{\tilde{X}}=\mathbf{U}^\top\mathbf{\breve{X}} = \underbrace{\boldsymbol{U}^{\top} \boldsymbol{U}}_{\boldsymbol{I}}\mathbf{\Sigma V}^\top =\mathbf{\Sigma V}^\top<br>$$</p>
<h4 id="2-Recounstruction"><a href="#2-Recounstruction" class="headerlink" title="2.Recounstruction"></a>2.Recounstruction</h4><p>$$<br>\mathbf{\breve{X}}\mathbf{V} = \mathbf{U\Sigma}\underbrace{\boldsymbol{V}^{\top} \boldsymbol{V}}_{\boldsymbol{I}} =\mathbf{U\Sigma}\<br>\Longrightarrow \mathbf{U}=\mathbf{\breve{X}}\mathbf{V\Sigma}^{-1}\<br>\begin{aligned}<br>\mathbf{\hat{X}}&amp;=\mathbf{U}\mathbf{\tilde{X}}+\mu\&amp;=\mathbf{\breve{X}}\mathbf{V\Sigma}^{-1}\mathbf{\tilde{X}}+\mu\&amp;=\mathbf{\breve{X}}\mathbf{V\Sigma}^{-1}\mathbf{\Sigma V}^\top+\mu\<br>&amp;=\mathbf{\breve{X}}\mathbf{VV}^\top+\mu<br>\end{aligned}<br>$$</p>
<h4 id="3-Test-Set"><a href="#3-Test-Set" class="headerlink" title="3. Test Set"></a>3. Test Set</h4><p>$$<br>\begin{array}{l}<br>\boldsymbol{U}=\check{\boldsymbol{X}} \boldsymbol{V} \Sigma^{-1} \Longrightarrow \boldsymbol{U}^{\top}=\Sigma^{-\top} \boldsymbol{V}^{\top} \breve{\boldsymbol{X}}^{\top}=\Sigma^{-1} \boldsymbol{V}^{\top} \breve{\boldsymbol{X}}^{\top} \<br>\tilde{\boldsymbol{X}}<em>{t}=\boldsymbol{U}^{\top} \breve{\boldsymbol{X}}</em>{t}=\Sigma^{-1} \boldsymbol{V}^{\top} \breve{\boldsymbol{X}}^{\top} \breve{\boldsymbol{X}}_{t}<br>\end{array}<br>$$</p>
<p>$$<br>\begin{aligned}<br>\boldsymbol{U} \boldsymbol{U}^{\top} &amp;=\breve{\boldsymbol{X}} \boldsymbol{V} \boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}^{-1} \boldsymbol{V}^{\top} \breve{\boldsymbol{X}} \ \widehat{\boldsymbol{X}}<em>{t}&amp;=\breve{\boldsymbol{X}} \boldsymbol{V} \boldsymbol{\Sigma}^{-2} \boldsymbol{V}^{\top} \breve{\boldsymbol{X}}^{\top}  \breve{\boldsymbol{X}}</em>{t}+\boldsymbol{\mu}_{x}<br>\end{aligned}<br>$$</p>

                </div>
                <hr/>

                <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone, qq, weibo, douban"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

                <div class="reprint">
                    <p>
                        <span class="reprint-tip">转载请注明: </span>
                        <a href="https://chitoseyono.com" class="b-link-green">White Noise -Chitose Yono Offical Blog-</a>
                        <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                        <a href="/2021/04/21/rl-pca/" class="b-link-green">表征学习、以及PCA的细节公式推导</a>
                    </p>
                </div>
            </div>
        </div>

        
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Wed Apr 21 2021 11:05:06 GMT+0800',
    owner: 'ChitoseYono',
    repo: 'chitoseyono.github.io',
    oauth: {
        client_id: '999f7c73059996845f36',
        client_secret: '8c466460b56a73683ea40833c9d5dca6f530ce9c'
    }
});

gitment.render('gitment-content');
</script>
        

        

        

<article id="articles" class="container prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">本篇</div>
            <div class="card">
                <a href="/2021/04/21/rl-pca/">
                    <div class="card-image">
                        
                        <img src="/medias/images/rl_pca.jpg" class="responsive-img" alt="表征学习、以及PCA的细节公式推导">
                        
                        <span class="card-title">表征学习、以及PCA的细节公式推导</span>
                    </div>

                    <div class="card-content article-content">
                        <div class="summary"> Representation Learning 表征学习
这章将稍微讲讲表征学习，以及经典方法PCA。
机器学习算法的与否可行不仅仅取决于算法的正确选用，也取决于数据的质量和有效的数据表示（representation）。针对不同类型的数</div>
                        <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-calendar fa-fw"></i>2021-04-21
                            </span>
                            <span class="publish-author">
                                <i class="fa fa-user fa-fw"></i>
                                
                                Chitose Yono
                                
                            </span>
                        </div>
                    </div>

                    
                    <div class="card-action article-tags">
                        
                        <a href="https://chitoseyono.com/tags/机器学习/"><span class="chip bg-color">机器学习</span></a>
                        
                    </div>
                    
                </a>
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">下一篇</div>
            <div class="card">
                <a href="/2021/03/19/ml-talk/">
                    <div class="card-image">
                        
                        <img src="/medias/images/ml_talk.jpg" class="responsive-img" alt="聊聊机器学习——学习、资源与前景">
                        
                        <span class="card-title">聊聊机器学习——学习、资源与前景</span>
                    </div>
                    <div class="card-content article-content">
                        <div class="summary">
                             
Hi，好久不见，这里是千歲世野。由于被各种琐事拖占，我已经很久没有在这里发过技术博客了，虽然偶尔我也会有记录学习过程或者做知识的积累，但是暂时大部分都还没有到能让我值得将他们分享的地步，这篇算是能发的地步，希望能帮助到大家。

这次的文
                        </div>
                        <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-calendar fa-fw"></i>2021-03-19
                            </span>
                            <span class="publish-author">
                                <i class="fa fa-user fa-fw"></i>
                                
                                Chitose Yono
                                
                            </span>
                        </div>
                    </div>
                    
                    <div class="card-action article-tags">
                        
                        <a href="https://chitoseyono.com/tags/机器学习/"><span class="chip bg-color">机器学习</span></a>
                        
                    </div>
                    
                </a>
            </div>
        </div>
        
    </div>
</article>
    </div>
</main>

<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;<a href="http://chitoseyono.com" target="_blank">&nbsp;Chitose Yono</a>&nbsp;|
            Published with <a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;|
            Theme by <a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">hexo-theme-matery</a>&nbsp;|
            Visited <span id="busuanzi_value_site_uv"></span> times<br>
        </div>
        <div class="col s12 m4 l4 social-link">
            <a href="https://github.com/chitoseyono" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
                <i class="fa fa-github fa-lg"></i>
            </a>
            <a href="mailto:chitoseyono@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
                <i class="fa fa-envelope fa-lg"></i>
            </a>
            <a href="https://weibo.com/2818513061/profile" class="tooltipped" data-tooltip="访问我的Weibo" data-position="top" data-delay="50">
                <i class="fa fa-weibo fa-lg"></i>
            </a>
        </div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title">搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-double-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link rel="stylesheet" href="/dist/APlayer.min.css">
<div id="aplayer"></div>
<script type="text/javascript" src="/dist/APlayer.min.js"></script>
<script type="text/javascript" src="/js/music.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>